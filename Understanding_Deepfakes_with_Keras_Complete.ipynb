{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Understanding Deepfakes with Keras - Complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/around-star/Deep-Fakes/blob/master/Understanding_Deepfakes_with_Keras_Complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8y91qUhJa4x",
        "colab_type": "text"
      },
      "source": [
        "# Understanding Deepfakes with Keras\n",
        "\n",
        "![DCGAN](DCGAN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vow7ijA7Ja4z",
        "colab_type": "text"
      },
      "source": [
        "# Task 1: Importing Libraries and Helper Functions\n",
        "\n",
        "Please note: If you haven't already, please install the required packages by executing the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqcEs-dLJa40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tensorflow==2.1.0 pillow matplotlib\n",
        "!pip3 install git+https://github.com/am1tyadav/tfutils.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eBoStgCOlD6V",
        "colab": {}
      },
      "source": [
        "%matplotlib notebook\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tfutils\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Reshape, LeakyReLU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from PIL import Image\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn_FVQa1Ja5B",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Importing and Plotting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_kAZDR5Ja5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tfutils.datasets.mnist.load_data(one_hot=False)\n",
        "\n",
        "x_train = tfutils.datasets.mnist.load_subset([0], x_train, y_train)\n",
        "x_test = tfutils.datasets.mnist.load_subset([0], x_test, y_test)\n",
        "\n",
        "x = np.concatenate([x_train, x_test], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vJrX749GlD6k",
        "colab": {}
      },
      "source": [
        "tfutils.datasets.mnist.plot_ten_random_examples(plt, x, np.zeros((x.shape[0], 1))).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j1Ns2oPJa5K",
        "colab_type": "text"
      },
      "source": [
        "# Task 3: Discriminator\n",
        "\n",
        "![Artist and Critic](artist_critic.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JHga1nihlD6q",
        "colab": {}
      },
      "source": [
        "size = 28\n",
        "noise_dim = 1\n",
        "\n",
        "discriminator = Sequential([\n",
        "    Conv2D(64, 3, strides=2, input_shape=(28, 28, 1)),\n",
        "    LeakyReLU(),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Conv2D(128, 5, strides=2),\n",
        "    LeakyReLU(),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Conv2D(256, 5, strides=2),\n",
        "    LeakyReLU(),\n",
        "    BatchNormalization(),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr=2e-4, beta_1=0.5)\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiRp6czFJa5Q",
        "colab_type": "text"
      },
      "source": [
        "# Task 4: Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-8mG_PlhlD6s",
        "colab": {}
      },
      "source": [
        "generator = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(noise_dim,)),\n",
        "    Reshape((1, 1, 256)),\n",
        "    \n",
        "    Conv2DTranspose(256, 5, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(128, 5, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2DTranspose(64, 5, strides=2, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2DTranspose(32, 5, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2DTranspose(1, 4, activation='sigmoid')\n",
        "\n",
        "])\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pSb3O5MDlD6u",
        "colab": {}
      },
      "source": [
        "noise = np.random.randn(1, noise_dim)\n",
        "gen_image = generator.predict(noise)[0]\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(np.reshape(gen_image, (28, 28)), cmap='binary');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0BSkkhsJa5a",
        "colab_type": "text"
      },
      "source": [
        "# Task 5: Generative Adversarial Network (GAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6YdMQyzclD69",
        "colab": {}
      },
      "source": [
        "input_layer = tf.keras.layers.Input(shape=(noise_dim,))\n",
        "gen_out = generator(input_layer)\n",
        "disc_out = discriminator(gen_out)\n",
        "\n",
        "gan = Model(\n",
        "    input_layer,\n",
        "    disc_out\n",
        ")\n",
        "\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "gan.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tha3OO1MJa5e",
        "colab_type": "text"
      },
      "source": [
        "# Tasks 6 and 7: Training the GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HEb3xQ0LlD6_",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "epochs = 25\n",
        "batch_size = 128\n",
        "steps_per_epoch = int(2 * x.shape[0]/batch_size)\n",
        "\n",
        "print('Steps per epoch=', steps_per_epoch)\n",
        "\n",
        "dp = tfutils.plotting.DynamicPlot(plt, 5, 5, (8, 8))\n",
        "\n",
        "for e in range(0, epochs):\n",
        "    \n",
        "    dp.start_of_epoch(e)\n",
        "    \n",
        "    for step in range(0, steps_per_epoch):\n",
        "        true_examples = x[int(batch_size/2)*step: int(batch_size/2)*(step + 1)]\n",
        "        true_examples = np.reshape(true_examples, (true_examples.shape[0], 28, 28, 1))\n",
        "\n",
        "        noise = np.random.randn(int(batch_size/2), noise_dim)\n",
        "        generated_examples = generator.predict(noise)\n",
        "\n",
        "        x_batch = np.concatenate([generated_examples, true_examples], axis=0)\n",
        "        y_batch = np.array([0] * int(batch_size/2) + [1] * int(batch_size/2))\n",
        "\n",
        "        indices = np.random.choice(range(batch_size), batch_size, replace=False)\n",
        "        x_batch = x_batch[indices]\n",
        "        y_batch = y_batch[indices]\n",
        "\n",
        "        # train the discriminator\n",
        "        discriminator.trainable = True\n",
        "        discriminator.train_on_batch(x_batch, y_batch)\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        # train the generator\n",
        "        loss, _ = gan.train_on_batch(noise, np.ones((int(batch_size/2), 1)))\n",
        "\n",
        "        _, acc = discriminator.evaluate(x_batch, y_batch, verbose=False)\n",
        "\n",
        "    noise = np.random.randn(1, noise_dim)\n",
        "    generated_example = generator.predict(noise)[0]\n",
        "    \n",
        "    dp.end_of_epoch(np.reshape(generated_example, (28, 28)), 'binary',\n",
        "                   'DiscAcc:{:.2f}'.format(acc), 'GANLoss:{:.2f}'.format(loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}